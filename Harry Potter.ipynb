{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv = gym.make(\"FrozenLake-v1\")\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "action_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space_size_rows=state_space_size-9\n",
    "state_space_size_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space_size_columns=state_space_size-8\n",
    "state_space_size_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_episodes = 15000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.2\n",
    "discount_rate = 0.95\n",
    "\n",
    "rewards_avg = []\n",
    "\n",
    "q_table = np.zeros((state_space_size_rows, state_space_size_columns))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average  0\n",
      "average  1\n",
      "average  2\n",
      "average  3\n",
      "average  4\n",
      "average  5\n",
      "average  6\n",
      "average  7\n",
      "average  8\n",
      "average  9\n",
      "average  10\n",
      "average  11\n",
      "average  12\n",
      "average  13\n",
      "average  14\n",
      "average  15\n",
      "average  16\n",
      "average  17\n",
      "average  18\n",
      "average  19\n",
      "average  20\n",
      "average  21\n",
      "average  22\n",
      "average  23\n",
      "average  24\n",
      "average  25\n",
      "average  26\n",
      "average  27\n",
      "average  28\n",
      "average  29\n",
      "average  30\n",
      "average  31\n",
      "average  32\n",
      "average  33\n",
      "average  34\n",
      "average  35\n",
      "average  36\n",
      "average  37\n",
      "average  38\n",
      "average  39\n",
      "average  40\n",
      "average  41\n",
      "average  42\n",
      "average  43\n",
      "average  44\n",
      "average  45\n",
      "average  46\n",
      "average  47\n",
      "average  48\n",
      "average  49\n",
      "average  50\n",
      "average  51\n",
      "average  52\n",
      "average  53\n",
      "average  54\n",
      "average  55\n",
      "average  56\n",
      "average  57\n",
      "average  58\n",
      "average  59\n",
      "average  60\n",
      "average  61\n",
      "average  62\n",
      "average  63\n",
      "average  64\n",
      "average  65\n",
      "average  66\n",
      "average  67\n",
      "average  68\n",
      "average  69\n",
      "average  70\n",
      "average  71\n",
      "average  72\n",
      "average  73\n",
      "average  74\n",
      "average  75\n",
      "average  76\n",
      "average  77\n",
      "average  78\n",
      "average  79\n",
      "average  80\n",
      "average  81\n",
      "average  82\n",
      "average  83\n",
      "average  84\n",
      "average  85\n",
      "average  86\n",
      "average  87\n",
      "average  88\n",
      "average  89\n",
      "average  90\n",
      "average  91\n",
      "average  92\n",
      "average  93\n",
      "average  94\n"
     ]
    }
   ],
   "source": [
    "# This cycle is to calculate the average reward/episodes and its only purpose is to plot the nice graph below that\n",
    "# shows how the agent learn how to maximize the reward.\n",
    "for it in range(100):\n",
    "    print('average ', it)\n",
    "    rewards_all_episodes=[]\n",
    "    \n",
    "    # exporation-exploitation trade-off params\n",
    "    exploration_rate = 1\n",
    "    max_exploration_rate = 1\n",
    "    min_exploration_rate = 0.01\n",
    "    exploration_decay_rate = 0.005\n",
    "    \n",
    "    # init q table in zeros\n",
    "    q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "    # iterate over the episodes\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "        \n",
    "        # iterate over the steps for an episode\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Exploration-exploitation trade-off\n",
    "            exploration_rate_threshold = random.uniform(0, 1)\n",
    "            if exploration_rate_threshold > exploration_rate:\n",
    "                action = np.argmax(q_table[state,:])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # Take action\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Update Q-table for Q(s,a)\n",
    "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "                learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "            # transition next state\n",
    "            state = new_state\n",
    "            rewards_current_episode += reward\n",
    "\n",
    "            if done == True: \n",
    "                break\n",
    "\n",
    "        # Exploration rate decay\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "    rewards_avg.append(rewards_all_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(0,num_episodes)]\n",
    "y = np.mean(rewards_avg, axis=0)\n",
    "plot.xlabel('Episodes')\n",
    "plot.ylabel('Reward')\n",
    "plot.plot(x, y,'o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
